# 如果 application.properties 和 application.yml 中有相同的配置，以 application.properties 的为准
debug: false

server:
  port: 8080

spring:
  # note: ---------------- 国际化配置 ----------------
  messages:
    # basename：基名，多个基名以逗号分隔（实质上是完全限定的类路径位置），
    # 每个基名都遵循 ResourceBundle 约定，对基于斜杠的位置提供了宽松的支持。如果它不包含包限定符（例如“org.mypackage”），则将从类路径根目录解析它。
    #基名可以理解为前缀，默认是从classpath根路径下找，配置的路径可以用斜杠/，也可以用.，
    # 即i18n/messages或i18n.messages，然后messages就是国际化文件的前缀，messages.properties就是默认国际化文件。
    basename: i18n/messages
    # fallback-to-system-locale：如果未找到特定区域设置的文件，是否回退到系统区域设置。
    # 如果关闭此功能，则唯一的回退将是默认文件。就是我想要的区域语言文件没有的时候，
    # 就从系统中解析系统的区域，以系统的区域再找一次文件，找到就返回系统区域文件，如果找不到，就返回默认的文件。
    fallback-to-system-locale: false

  # 数据库配置
  datasource:
    url: jdbc:mysql://192.168.2.100:3306/vonsiking?serverTimezone=Asia/Shanghai&useUnicode=true&characterEncoding=utf-8&useSSL=true
    username: root
    password: feng234800
    driver-class-name: com.mysql.cj.jdbc.Driver

  kafka:
    bootstrap-servers: 192.168.2.100:9092
    producer:
      # 发生错误后，消息重发的次数。
      retries: 0
      #当有多个消息需要被发送到同一个分区时，生产者会把它们放在同一个批次里。该参数指定了一个批次可以使用的内存大小，按照字节数计算。
      batch-size: 16384
      # 设置生产者内存缓冲区的大小。
      buffer-memory: 33554432
      # 键的序列化方式
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      # 值的序列化方式
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      # acks=0 ： 生产者在成功写入消息之前不会等待任何来自服务器的响应。
      # acks=1 ： 只要集群的首领节点收到消息，生产者就会收到一个来自服务器成功响应。
      # acks=all ：只有当所有参与复制的节点全部收到消息时，生产者才会收到一个来自服务器的成功响应。
      acks: 1
    consumer:
      # 这个配置是必须的，否则启动时会报错，值是我随便写了个
      group-id: 111
      # 自动提交的时间间隔 在spring boot 2.X 版本中这里采用的是值的类型为Duration 需要符合特定的格式，如1S,1M,2H,5D
      auto-commit-interval: 1S
      # 该属性指定了消费者在读取一个没有偏移量的分区或者偏移量无效的情况下该作何处理：
      # latest（默认值）在偏移量无效的情况下，消费者将从最新的记录开始读取数据（在消费者启动之后生成的记录）
      # earliest ：在偏移量无效的情况下，消费者将从起始位置读取分区的记录
      auto-offset-reset: earliest
      # 是否自动提交偏移量，默认值是true,为了避免出现重复数据和数据丢失，可以把它设置为false,然后手动提交偏移量
      enable-auto-commit: false
      # 键的反序列化方式
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      # 值的反序列化方式
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
    listener:
      # 在侦听器容器中运行的线程数。
      concurrency: 5
      #listner负责ack，每调用一次，就立即commit
      ack-mode: manual_immediate
      missing-topics-fatal: false

  data:
    redis:
      host: 192.168.2.100
      port: 6379 # Redis服务器连接密码（默认为空）
      password:  # Redis服务器连接密码（默认为空）
      database: 0 # Redis数据库索引（默认为0）
      client-type: lettuce
      lettuce:
        pool:
          max-active: 8  # 连接池最大连接数（使用负值表示没有限制），默认 8
          max-wait: -1ms  # 连接池最大阻塞等待时间（使用负值表示没有限制），默认 -1
          max-idle: 8  # 连接池中的最大空闲连接，默认 8
          min-idle: 1  # 连接池中的最小空闲连接，默认 0
      timeout: 3000ms  # 连接超时时间（毫秒）

  # low level低级别的已过期的操作方式。
  elasticsearch:
    rest:
      uris: 192.168.2.100:9200
      connection-timeout: 1s
      read-timeout: 30s

mybatis:
  mapper-locations: classpath:sqlMap/*.xml

# elasticsearch配置
elasticsearch:
  # 自定义属性---设置是否开启ES,false表不开窍ES
  open: true
  # es集群名称,如果下载es设置了集群名称,则使用配置的集群名称
  clusterName: es
  hosts: 192.168.2.100:9200
  # es 请求方式
  scheme: http
  # es 连接超时时间
  connectTimeOut: 1000
  # es socket 连接超时时间
  socketTimeOut: 30000
  # es 请求超时时间
  connectionRequestTimeOut: 600
  # es 最大连接数
  maxConnectNum: 100
  # es 每个路由的最大连接数
  maxConnectNumPerRoute: 100

